{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersubject Correlation: Synchronizing our Social Cognition (Part 2)\n",
    "In the previous notebook [isc-generation](https://github.com/lindseytepfer/psypose-isc/blob/main/code/isc-generation.ipynb), we correlated individual subjects with the rest of the individuals that watched the same movies that they did, performing this correlation every 20 seconds throughout the entire length of the movie. Then we averaged the subjects together (within each movie), so that we could ultimately model that data using a general linear model. \n",
    "\n",
    "However, before we get to the model, we'll need to prepare a few things in advance: namely, getting the regressors themselves in order, and then before we create a design matrix, we'll need to ensure that the regressors and averaged-isc data are the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys\n",
    "tasknames = ['12yearsaslave','500daysofsummer','backtothefuture','citizenfour',\n",
    "           'littlemisssunshine', 'pulpfiction','split','theprestige',\n",
    "           'theshawshankredemption','theusualsuspects']\n",
    "#values\n",
    "vidnames = ['12_years_a_slave','500_days_of_summer','back_to_the_future','citizenfour',\n",
    "           'little_miss_sunshine', 'pulp_fiction','split','the_prestige',\n",
    "           'the_shawshank_redemption','the_usual_suspects']\n",
    "\n",
    "zippedlist = zip(tasknames,vidnames)\n",
    "tasktovidmap = dict(zippedlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Change Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isc_outs = \"/Volumes/Scraplab/psypose_fmri/isc_analysis/\"\n",
    "\n",
    "for task in tasknames:\n",
    "    diar_df = pd.read_csv(isc_outs+task+os.sep+task+\"_diarization_cleaned.csv\")\n",
    "    pose_df = pd.read_csv(isc_outs+task+os.sep+task+\"_regressor.csv\")\n",
    "    \n",
    "    track_timelines = []    \n",
    "    speaker_list = []\n",
    "\n",
    "    for i in range(diar_df.index.max()+1):\n",
    "        start, stop = int(diar_df.loc[i, \"start\"]), int(diar_df.loc[i, \"stop\"])\n",
    "        speaker = diar_df.loc[i, \"speaker\"]\n",
    "        for r in range (start,stop+1):\n",
    "            track_timelines.append(r)\n",
    "            speaker_list.append(speaker)\n",
    "    \n",
    "    zippedlist = zip(track_timelines,speaker_list)\n",
    "    speaker_tracks = dict(zippedlist)\n",
    "    \n",
    "    df = pd.DataFrame({\"seconds\":pose_df.index})\n",
    "    df[[\"speaker_change\",\"speaker\"]] = 0,0\n",
    "\n",
    "    for t in speaker_tracks:\n",
    "        for i in range(df.index.max()+1):\n",
    "            if df.loc[i,\"seconds\"] == t:\n",
    "                df.loc[i,\"speaker\"] = speaker_tracks[t]\n",
    "                \n",
    "    df2 = df.loc[df.speaker != 0]\n",
    "    df2_index = df2.index\n",
    "    \n",
    "    for a,b in enumerate(df2_index):\n",
    "        try: \n",
    "            if df2.loc[b,\"speaker\"] != df2.loc[df2_index[a+1], \"speaker\"]:\n",
    "                df2.loc[df2_index[a+1], \"speaker_change\"] = 1\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    speaker_changes = df2[\"speaker_change\"].reindex(range(df.index.max()), fill_value= '0')\n",
    "    df[\"speaker_change\"] = speaker_changes\n",
    "    df.to_csv(isc_outs+task+os.sep+task+\"_speaker_change.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a GLM\n",
    "Now that all of our regressors have been generated, and we made sure that the shapes between regressors and average-isc data match, we can now run our GLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_betas = []\n",
    "TR = 1\n",
    "\n",
    "for task in tasknames[0:1]:\n",
    "    #First, load the movie's average ISC-this is our dependent variable.\n",
    "    avg_isc_nb = nb.load(isc_outs+task+\"/\"+task+\"_average_isc.nii.gz\")\n",
    "    avg_isc_trimmed = avg_isc_nb.slicer[...,:avg_isc_nb.shape[3]-20] #trims 20 seconds off the end since the rolling window inherently extends beyond valid correlations.\n",
    "    avg_isc = Brain_Data(avg_isc_trimmed)\n",
    "    \n",
    "    #Next, because of the naming mis-match, we grab the subject's task name from the first file to map it to the video\n",
    "    video = \"\".join([val for key,val in tasktovidmap.items() if key in task])\n",
    "    \n",
    "    #Now we can load that task video and convolve it\n",
    "    vid_regr = pd.read_csv(isc_outs+task+\"/\"+video+'_person_regr_shifted.csv')\n",
    "    dm = Design_Matrix(vid_regr, sampling_freq=1./TR) \n",
    "    dm = dm.convolve()\n",
    "    dm = dm.add_poly(order=0)\n",
    "    \n",
    "    #We ensure that any NaNs in our design matrix are filled, and remove columns with duplicate data \n",
    "    dm_cleaned = dm.clean(verbose=True)\n",
    "    dm_cleaned.to_csv(isc_outs+task+os.sep+task+\"_design_matrix.csv\")\n",
    "\n",
    "    #Set the design matrix, full_dm_cleaned, to the X attribute of the brain data object (subj_run_data)\n",
    "    avg_isc.X = dm_cleaned\n",
    "    stats = avg_isc.regress()\n",
    "    \n",
    "    #write our results to a beta map nii file. \n",
    "    stats['beta'].write(isc_outs+task+\"/\"+video+'_betamap.nii.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
