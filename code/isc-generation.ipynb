{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Intersubject Correlation: Synchronizing our Social Cognition\n","**Summary**: In this project, [we](http://scraplab.org/) are interested in examining how neural synchrony among participants may be elicited for different categories of social stimuli. For instance, will we see an increase in neural synchrony when there the number of characters present on-screen increases? In addition to the visual aspect--participants, as a part of the [Naturalistic Neuroimaging Database](https://www.naturalistic-neuroimaging-database.org/) (NNDb) paradigm are watching one of ten movies--we also consider some elements of speech, such as when overlap occurs, or how emotional the speech is. \n","\n","First, we install the packages used in this stage of the analysis. We also use several deep learning packages to annotate various features of the NNDb movies, which we cite as the data is incorporated into the pipeline. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","import glob\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["datapath='/Volumes/Scraplab/data/ds002837/derivatives/'\n","isc_outs = '/Volumes/Scraplab/psypose_fmri/isc_analysis/'\n","\n","#Generate the subject list and filenames\n","func_data = os.listdir(datapath)\n","sub_ids = [x for x in func_data if ('sub-') in x] #grab all the subject IDs for easy filtering\n","\n","#net together all the datafiles independent of which task they are from\n","all_task_subs = [] \n","for id in sub_ids:\n","    all_task_subs.append(glob.glob(os.path.join(datapath+id+'/func/*blur_censor_ica.nii.gz'))[0])\n","\n","#Listing the 10 different movie stimuli \n","tasknames = ['12yearsaslave','500daysofsummer','backtothefuture','citizenfour',\n","           'littlemisssunshine', 'pulpfiction','split','theprestige',\n","           'theshawshankredemption','theusualsuspects']"]},{"cell_type":"markdown","metadata":{},"source":["## Summing the functional timecourses\n","\n","First, for each movie, we sum up all the participants' fMRI timecourses, creating one giant matrix. This is so that later on in the pipeline we can subtract out a single subject of the group, and then correlate that single subject's timecourse with the summed timecourse of the rest of the participants."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for task in tasknames:\n","    #retrieve the task's nii files\n","    task_files = [x for x in all_task_subs if task in x]\n","    one_subj = nb.load(task_files[0]) #pull a single subject to grab the film length (the # of TRs)\n","    film_length = one_subj.shape[3] #grab that 3rd dimension, the TR\n","    \n","    #create an empty matrix the size of all the subject timecourses \n","    group_brain = np.empty([len(task_files),64,76,64,film_length])\n","    index=0\n","    \n","    #now we go into the list of the current movie's participants \n","    for file in task_files:  \n","        one_nii = nb.load(file) #load each file\n","        group_brain[index,:,:,:,:] = np.array(one_nii.get_fdata(),dtype=np.float64) # retrieve the functional data and 'index it' into the location that corresponds to the subject\n","        index += 1 #update the index \n","    \n","    #this loop takes about 50 minutes per movie. \n","    group_brain_sum = group_brain.sum(axis=0)\n","    #affix header and affine data to the averaged nifti file\n","    array_img = nb.Nifti1Image(group_brain_sum,one_subj.affine,one_subj.header)\n","    nb.save(array_img,\"isc_analysis/\"+task+\"/\"+task+\"_summed_timecourse.nii.gz\")"]},{"cell_type":"markdown","metadata":{},"source":["## Creating a Mask for our Data\n","We want to mask our data so that we don't inadvertently correlate data outside of the boundaries of the brain - not only is this unecessary, it can also cause a number of issues further on in our analysis."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["global_mask_list = []\n","\n","for subject in all_task_subs: \n","    subject_nb = nb.load(subject)\n","    subject_arr = subject_nb.get_fdata()\n","    subject_mask = np.mean(subject_arr != 0, axis=3) \n","    global_mask_list.append(subject_mask) #subject_mask shape: (64,76,64)\n","\n","np.save(isc_outs+'global_mask_list', global_mask_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mask_np = np.load(isc_outs+'global_mask_list.npy')\n","global_mask = mask_np != 0\n","global_mask_med = np.median(global_mask,axis=0)\n","np.save(isc_outs+'global_mask', global_mask_med)"]},{"cell_type":"markdown","metadata":{},"source":["## Correlating Subject Data\n","Now we are onto the correlation itself. For each movie, we load the summed data that we created earlier, and subtract out one subject. We take that subject's data and correlate it with the remaining participants' data. We perform this correlation as a sliding window-every 20 seconds we take a new correlation, and we do this throughout the entire length of the movie. This is a very computationally intensive process, so we advise you to parallelize this stage of the pipeline. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mask = np.load(isc_outs+'global_mask.npy')\n","\n","for task in tasknames: \n","    subject_list = [x for x in func_data if task in x]\n","    summed_timecourse_nb = nb.load(isc_outs+task+'/'+task+'_summed_timecourse.nii.gz')\n","    summed_timecourse_arr = summed_timecourse_nb.get_fdata()\n","    movie_tr_length = summed_timecourse_nb.shape[3]\n","    inter_subject_correlation = np.empty(summed_timecourse_nb.shape)\n","\n","    for i in subject_list:\n","        sub_id = i.split(\"/\")[6]\n","        sub_nb = nb.load(i)\n","        sub_arr = sub_nb.get_fdata()\n","        xn, yn, zn = sub_arr.shape[0], sub_arr.shape[1], sub_arr.shape[2]\n","        header, affine = sub_nb.header, sub_nb.affine\n","        group_data = summed_timecourse_arr - sub_arr\n","\n","        for tr in range(movie_tr_length):\n","            single_subject_window = sub_arr[:,:,:,tr:tr+20]\n","            group_data_window = group_data[:,:,:,tr:tr+20]\n","\n","            for x in range(xn):\n","                for y in range(yn):\n","                    for z in range(zn):\n","                        if mask[x,y,z]:\n","                            inter_subject_correlation[x,y,z,tr] = np.corrcoef(single_subject_window[x,y,z].flatten(),group_data_window[x,y,z].flatten(), rowvar=False)[0,1]\n","                        else:\n","                            continue\n","\n","        array_img = nb.Nifti1Image(inter_subject_correlation,affine,header)\n","        nb.save(array_img,isc_outs+task+\"/\"+task+\"_\"+sub_id+\"_correlated_timeseries.nii.gz\")"]},{"cell_type":"markdown","metadata":{},"source":["## Averaging ISC Data\n","\n","Next, we take the average of each movie's ISC data. We will model this data with a general linear model, as shown in the notebook isc-modeling. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for task in tasknames: \n","    isc_dir = os.listdir(isc_outs+task) \n","    all_isc_files = [x for x in isc_dir if ('correlated_timeseries.nii.gz') in x]\n","    subject_iscs = [x for x in all_isc_files if task in x]\n","    \n","    #create an empty matrix the size of all the subject timecoursess\n","    dim_sub = nb.load('/Volumes/Scraplab/psypose_fmri/isc_analysis/'+task+os.sep+\"\".join(all_subject_iscs[0:1]))\n","    isc_data_arr = np.empty([len(all_subject_iscs),dim_sub.shape])\n","    index = 0\n","    \n","    for subject in subject_iscs:\n","        subject_nb = nb.load('/Volumes/Scraplab/psypose_fmri/single_subject_correlations/'+subject)\n","        isc_data_arr[index,:,:,:,:] = np.array(subject_nb.get_fdata(),dtype=np.float64)\n","        index += 1\n","    \n","    avg_isc = isc_data_arr.mean(axis=0)\n","    array_img = nb.Nifti1Image(avg_isc,dim_sub.affine,dim_sub.header)\n","    nb.save(array_img,isc_outs+task+\"/\"+task+\"_average_isc.nii.gz\")"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
